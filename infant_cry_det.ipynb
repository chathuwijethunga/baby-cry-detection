{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb945ac",
   "metadata": {},
   "source": [
    "For infant cry detection, you can use various machine learning models to train your classifier. The choice of model depends on your dataset, feature engineering, and performance requirements. Here are some common machine learning models you can consider for infant cry detection:\n",
    "\n",
    "1. **Random Forest Classifier:** As you've already used, Random Forest is an ensemble learning method that can work well for classification tasks. It can handle both numerical and categorical features and is robust against overfitting.\n",
    "\n",
    "2. **Support Vector Machine (SVM):** SVM is a powerful classifier that can be effective in binary classification tasks like cry detection. It tries to find the hyperplane that best separates the two classes.\n",
    "\n",
    "3. **Logistic Regression:** Logistic regression is a simple yet effective model for binary classification. It's easy to interpret and can serve as a baseline model.\n",
    "\n",
    "4. **K-Nearest Neighbors (K-NN):** K-NN is a non-parametric algorithm that classifies an instance based on the majority class among its k-nearest neighbors. It's easy to understand and can work well with appropriate distance metrics.\n",
    "\n",
    "5. **Gradient Boosting (e.g., XGBoost, LightGBM):** Gradient boosting algorithms can provide high accuracy and can handle complex relationships in the data. They are particularly useful when dealing with imbalanced datasets.\n",
    "\n",
    "6. **Neural Networks:** Deep learning models, such as convolutional neural networks (CNNs) or recurrent neural networks (RNNs), can be employed for audio-based classification tasks like cry detection. They require a larger amount of data and computational resources but can achieve state-of-the-art performance.\n",
    "\n",
    "7. **Naive Bayes:** Naive Bayes classifiers are simple probabilistic models that can be suitable for text-based cry detection when text features are used.\n",
    "\n",
    "8. **Ensemble Methods:** You can create an ensemble of different classifiers to improve overall performance. For example, you can combine the predictions of multiple models like Random Forest, SVM, and Logistic Regression.\n",
    "\n",
    "9. **Hidden Markov Models (HMMs):** HMMs are commonly used for time-series data and speech recognition. They can be adapted for audio-based cry detection.\n",
    "\n",
    "The choice of model depends on the complexity of your dataset, the quality of your features, and your computational resources. It's often a good idea to start with simpler models (e.g., Random Forest or Logistic Regression) and gradually explore more complex models if needed. Additionally, consider using techniques like feature engineering, hyperparameter tuning, and cross-validation to optimize your model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c68c3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import read\n",
    "import scipy.signal as signal\n",
    "import matplotlib as mpl\n",
    "from scipy.stats import skew, kurtosis\n",
    "cmap = mpl.colormaps['Reds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f723fc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_42484\\3213013389.py:1: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  sampling_rate,data= read(r\"C:\\Users\\Admin\\Downloads\\baby-crying-01.wav\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sampling_rate,data= read(r\"C:\\Users\\Admin\\Downloads\\baby-crying-01.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3c3a2485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(816000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "148c4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav=data[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dfe2bf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(wav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d15dbd17",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(wav)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(wav)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4122114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, t, Zxx = signal.stft(wav, fs=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2d0761b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(129,)\n"
     ]
    }
   ],
   "source": [
    "print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "46fd0365",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mpcolormesh(t, f, np\u001b[38;5;241m.\u001b[39mabs(Zxx), cmap\u001b[38;5;241m=\u001b[39mcmap)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.pcolormesh(t, f, np.abs(Zxx), cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ac39e26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import spectrogram\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import skew, kurtosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7447645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the infant cry audio files\n",
    "data_dir_cry = r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\301 - Crying baby\"\n",
    "\n",
    "# List all files in the directory\n",
    "audio_files_cry = os.listdir(data_dir_cry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ba31cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the infant non-cry audio files\n",
    "data_dir_non_cry = r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\901 - Silence\"\n",
    "\n",
    "# List all files in the directory\n",
    "audio_files_non_cry = os.listdir(data_dir_non_cry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "15da093d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.00000000e+00 1.11425845e-06 7.17823286e-06 4.07754356e-04\n",
      " 9.03873143e-10 1.97792342e-05 3.10823634e-05 3.85379485e+00\n",
      " 2.10503386e+01 4.07754356e-04 3.02229075e-12 2.66033549e-05\n",
      " 8.19006018e-05 3.75852090e+00 1.33049486e+01 2.64877472e-05\n",
      " 2.61818209e-13 2.18220225e-06 6.11661335e-06 3.29270060e+00\n",
      " 9.60648201e+00]\n"
     ]
    }
   ],
   "source": [
    "# Sample rate and other parameters\n",
    "fs = 500  # Sample rate \n",
    "nperseg = 64  # Window size for STFT \n",
    "noverlap = 8 # Overlap between windows \n",
    "\n",
    "# Initialize empty lists for features\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "# List all files in the directory\n",
    "#audio_files_cry = os.listdir(data_dir_cry)\n",
    "#audio_files_non_cry = os.listdir(data_dir_non_cry)\n",
    "\n",
    "# Loop through each cry audio file\n",
    "for audio_file in audio_files_cry:\n",
    "    # Check if the file is a WAV file\n",
    "    if audio_file.endswith(\".wav\"):\n",
    "        # Construct the full path to the audio file\n",
    "        audio_path = os.path.join(data_dir_cry, audio_file)\n",
    "        \n",
    "        # Read the audio file\n",
    "        fs, audio_signal = wavfile.read(audio_path)\n",
    "\n",
    "        # Compute STFT\n",
    "        f, t, Sxx = spectrogram(audio_signal, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "\n",
    "        # Calculate the 20 features from STFT\n",
    "        tf_mean = np.mean(Sxx)\n",
    "        tf_std = np.std(Sxx)\n",
    "        \n",
    "        tma = np.max(Sxx, axis=0)\n",
    "        tma_max = np.max(tma)\n",
    "        tma_min = np.min(tma)\n",
    "        tma_mean = np.mean(tma)\n",
    "        tma_std = np.std(tma)\n",
    "        tma_skewness = skew(tma)\n",
    "        tma_kurt = kurtosis(tma)\n",
    "        \n",
    "        fma = np.max(Sxx, axis=1)\n",
    "        fma_max = np.max(fma)\n",
    "        fma_min = np.min(fma)\n",
    "        fma_mean = np.mean(fma)\n",
    "        fma_std = np.std(fma)\n",
    "        fma_skewness = skew(fma)\n",
    "        fma_kurt = kurtosis(fma)\n",
    "        \n",
    "        fsda = np.std(Sxx, axis=1)\n",
    "        fsda_max = np.max(fsda)\n",
    "        fsda_min = np.min(fsda)\n",
    "        fsda_mean = np.mean(fsda)\n",
    "        fsda_std = np.std(fsda)\n",
    "        fsda_skewness = skew(fsda)\n",
    "        fsda_kurt = kurtosis(fsda)\n",
    "\n",
    "        # Determine if it's a cry or non-cry sound based on file name or other criteria\n",
    "        # For example, you can use a naming convention or other metadata to label the data\n",
    "        is_cry = 1 #if \"cry\" in audio_file else 0\n",
    "\n",
    "        # Combine features into a feature vector\n",
    "        feature_vector = np.array([is_cry, tf_mean, tf_std,\n",
    "                                   tma_max, tma_min, tma_mean, tma_std, tma_skewness, tma_kurt,\n",
    "                                   fma_max, fma_min, fma_mean, fma_std, fma_skewness, fma_kurt,\n",
    "                                   fsda_max, fsda_min, fsda_mean, fsda_std, fsda_skewness, fsda_kurt])\n",
    "\n",
    "        # Append feature vector and label to lists\n",
    "        features.append(feature_vector)\n",
    "        labels.append(is_cry)\n",
    "        \n",
    "print(feature_vector)       \n",
    "      \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "56f5d65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 4.25156479e-08 1.14318391e-06 2.06512152e-04\n",
      " 1.15668874e-09 1.18215473e-06 6.43134626e-06 1.90275364e+01\n",
      " 4.53019300e+02 2.06512152e-04 1.01963176e-08 7.01154886e-06\n",
      " 3.53567666e-05 5.43749967e+00 2.77179056e+01 6.43170142e-06\n",
      " 7.60432162e-10 2.23845490e-07 1.10258861e-06 5.40278890e+00\n",
      " 2.74526798e+01]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each audio file in the non-cry directory\n",
    "for audio_file in audio_files_non_cry:\n",
    "    # Check if the file is a WAV file\n",
    "    if audio_file.endswith(\".wav\"):\n",
    "        # Construct the full path to the audio file\n",
    "        audio_path = os.path.join(data_dir_non_cry, audio_file)\n",
    "        \n",
    "        # Read the audio file\n",
    "        fs, audio_signal = wavfile.read(audio_path)\n",
    "\n",
    "        # Compute STFT\n",
    "        f, t, Sxx = spectrogram(audio_signal, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "\n",
    "        # Calculate the 20 features from STFT\n",
    "        tf_mean = np.mean(Sxx)\n",
    "        tf_std = np.std(Sxx)\n",
    "        \n",
    "        tma = np.max(Sxx, axis=0)\n",
    "        tma_max = np.max(tma)\n",
    "        tma_min = np.min(tma)\n",
    "        tma_mean = np.mean(tma)\n",
    "        tma_std = np.std(tma)\n",
    "        tma_skewness = skew(tma)\n",
    "        tma_kurt = kurtosis(tma)\n",
    "        \n",
    "        fma = np.max(Sxx, axis=1)\n",
    "        fma_max = np.max(fma)\n",
    "        fma_min = np.min(fma)\n",
    "        fma_mean = np.mean(fma)\n",
    "        fma_std = np.std(fma)\n",
    "        fma_skewness = skew(fma)\n",
    "        fma_kurt = kurtosis(fma)\n",
    "        \n",
    "        fsda = np.std(Sxx, axis=1)\n",
    "        fsda_max = np.max(fsda)\n",
    "        fsda_min = np.min(fsda)\n",
    "        fsda_mean = np.mean(fsda)\n",
    "        fsda_std = np.std(fsda)\n",
    "        fsda_skewness = skew(fsda)\n",
    "        fsda_kurt = kurtosis(fsda)\n",
    "\n",
    "        # Label non-cry audio as 0\n",
    "        is_cry = 0\n",
    "\n",
    "        # Combine features into a feature vector\n",
    "        feature_vector = np.array([is_cry, tf_mean, tf_std,\n",
    "                                   tma_max, tma_min, tma_mean, tma_std, tma_skewness, tma_kurt,\n",
    "                                   fma_max, fma_min, fma_mean, fma_std, fma_skewness, fma_kurt,\n",
    "                                   fsda_max, fsda_min, fsda_mean, fsda_std, fsda_skewness, fsda_kurt])\n",
    "\n",
    "        # Append feature vector and label to lists\n",
    "        features.append(feature_vector)\n",
    "        labels.append(is_cry)\n",
    "print(feature_vector)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c2517e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame - column names\n",
    "column_names = ['IsCry', 'TF_Mean', 'TF_Std', 'TMA_Max', 'TMA_Min', 'TMA_Mean', 'TMA_Std', 'TMA_Skewness', 'TMA_Kurtosis',\n",
    "                'FMA_Max', 'FMA_Min', 'FMA_Mean', 'FMA_Std', 'FMA_Skewness', 'FMA_Kurtosis',\n",
    "                'FSDA_Max', 'FSDA_Min', 'FSDA_Mean', 'FSDA_Std', 'FSDA_Skewness', 'FSDA_Kurtosis']\n",
    "\n",
    "df = pd.DataFrame(features, columns=column_names)\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X = df.drop('IsCry', axis=1)\n",
    "y = df['IsCry']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a051863",
   "metadata": {},
   "source": [
    "Train a Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "089c7d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        23\n",
      "         1.0       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropna(inplace=True)  # Drops rows with missing values\n",
    "\n",
    "# Initialize and train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_path = \"cry_detection_model.pkl\"\n",
    "joblib.dump(clf, model_path)\n",
    "\n",
    "# Fit the model with feature names\n",
    "clf.feature_names = feature_vector \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "89e8a04d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23,  0],\n",
       "       [ 0, 13]], dtype=int64)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1a04abe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "The new audio clip does not contain a cry sound.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\cry_det\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import spectrogram\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Function to extract features from an audio file\n",
    "def extract_features(audio_path):\n",
    "    # Sample rate and other parameters (should match the training parameters)\n",
    "    fs = 500\n",
    "    nperseg = 64\n",
    "    noverlap = 8\n",
    "\n",
    "    # Read the audio file\n",
    "    fs, audio_signal = wavfile.read(audio_path)\n",
    "\n",
    "    # Compute STFT\n",
    "    f, t, Sxx = spectrogram(audio_signal, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "\n",
    "    # Calculate the 20 features from STFT\n",
    "    tf_mean = np.mean(Sxx)\n",
    "    tf_std = np.std(Sxx)\n",
    "    \n",
    "    tma = np.max(Sxx, axis=0)\n",
    "    tma_max = np.max(tma)\n",
    "    tma_min = np.min(tma)\n",
    "    tma_mean = np.mean(tma)\n",
    "    tma_std = np.std(tma)\n",
    "    tma_skewness = skew(tma)\n",
    "    tma_kurt = kurtosis(tma)\n",
    "        \n",
    "    fma = np.max(Sxx, axis=1)\n",
    "    fma_max = np.max(fma)\n",
    "    fma_min = np.min(fma)\n",
    "    fma_mean = np.mean(fma)\n",
    "    fma_std = np.std(fma)\n",
    "    fma_skewness = skew(fma)\n",
    "    fma_kurt = kurtosis(fma)\n",
    "        \n",
    "    fsda = np.std(Sxx, axis=1)\n",
    "    fsda_max = np.max(fsda)\n",
    "    fsda_min = np.min(fsda)\n",
    "    fsda_mean = np.mean(fsda)\n",
    "    fsda_std = np.std(fsda)\n",
    "    fsda_skewness = skew(fsda)\n",
    "    fsda_kurt = kurtosis(fsda)\n",
    "\n",
    "    # Determine if it's a cry or non-cry sound based on file name or other criteria\n",
    "    is_cry = 1 if \"cry\" in audio_file else 0\n",
    "\n",
    "    # Combine features into a feature vector\n",
    "    feature_vector = np.array([ tf_mean, tf_std,\n",
    "                               tma_max, tma_min, tma_mean, tma_std, tma_skewness, tma_kurt,\n",
    "                               fma_max, fma_min, fma_mean, fma_std, fma_skewness, fma_kurt, \n",
    "                               fsda_max, fsda_min, fsda_mean, fsda_std, fsda_skewness, fsda_kurt])\n",
    "\n",
    "    # Append feature vector and label to lists\n",
    "    features.append(feature_vector)\n",
    "    labels.append(is_cry)\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "# Path to the new audio clip \n",
    "new_audio_path = r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\903 - Baby laugh\\laugh_1.m4a_9.wav\"\n",
    "\n",
    "\n",
    "#baby laugh\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\903 - Baby laugh\\laugh_1.m4a_9.wav\"\n",
    "\n",
    "#baby cry\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\301 - Crying baby\\V_2017-04-01+08_06_22=0_30.mp3_12.wav\"\n",
    "\n",
    "#silence\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\901 - Silence\\silence.wav_8.wav\"\n",
    "\n",
    "# Extract features from the new audio clip\n",
    "new_feature_vector = extract_features(new_audio_path)\n",
    "\n",
    "# Make a prediction using the trained model\n",
    "prediction = clf.predict([new_feature_vector])\n",
    "print(prediction)\n",
    "# Map prediction to class label (you can define a mapping if needed)\n",
    "#class_label = \"Cry Sound\" if prediction == 1 else \"Non-Cry Sound\"\n",
    "\n",
    "# Print the result\n",
    "#print(f\"The audio clip is classified as: {class_label}\")\n",
    "# Interpret the prediction\n",
    "if prediction[0] == 1:\n",
    "    print(\"The new audio clip contains a cry sound.\")\n",
    "else:\n",
    "    print(\"The new audio clip does not contain a cry sound.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "12b82bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Louise_01.m4a_0.wav' - not a cry sound.\n",
      " 'Louise_01.m4a_1.wav' - a cry sound.\n",
      " 'Louise_01.m4a_10.wav' - a cry sound.\n",
      " 'Louise_01.m4a_11.wav' - a cry sound.\n",
      " 'Louise_01.m4a_12.wav' - a cry sound.\n",
      " 'Louise_01.m4a_13.wav' - a cry sound.\n",
      " 'Louise_01.m4a_14.wav' - a cry sound.\n",
      " 'Louise_01.m4a_2.wav' - a cry sound.\n",
      " 'Louise_01.m4a_3.wav' - a cry sound.\n",
      " 'Louise_01.m4a_4.wav' - a cry sound.\n",
      " 'Louise_01.m4a_5.wav' - a cry sound.\n",
      " 'Louise_01.m4a_6.wav' - a cry sound.\n",
      " 'Louise_01.m4a_7.wav' - a cry sound.\n",
      " 'Louise_01.m4a_8.wav' - a cry sound.\n",
      " 'Louise_01.m4a_9.wav' - a cry sound.\n",
      " 'margot.m4a_0.wav' - a cry sound.\n",
      " 'margot.m4a_1.wav' - a cry sound.\n",
      " 'margot.m4a_10.wav' - a cry sound.\n",
      "'margot.m4a_11.wav' - not a cry sound.\n",
      " 'margot.m4a_12.wav' - a cry sound.\n",
      " 'margot.m4a_13.wav' - a cry sound.\n",
      " 'margot.m4a_14.wav' - a cry sound.\n",
      " 'margot.m4a_15.wav' - a cry sound.\n",
      " 'margot.m4a_16.wav' - a cry sound.\n",
      " 'margot.m4a_17.wav' - a cry sound.\n",
      " 'margot.m4a_18.wav' - a cry sound.\n",
      " 'margot.m4a_19.wav' - a cry sound.\n",
      " 'margot.m4a_2.wav' - a cry sound.\n",
      " 'margot.m4a_20.wav' - a cry sound.\n",
      " 'margot.m4a_21.wav' - a cry sound.\n",
      " 'margot.m4a_22.wav' - a cry sound.\n",
      "'margot.m4a_23.wav' - not a cry sound.\n",
      "'margot.m4a_24.wav' - not a cry sound.\n",
      "'margot.m4a_25.wav' - not a cry sound.\n",
      " 'margot.m4a_26.wav' - a cry sound.\n",
      " 'margot.m4a_3.wav' - a cry sound.\n",
      " 'margot.m4a_4.wav' - a cry sound.\n",
      " 'margot.m4a_5.wav' - a cry sound.\n",
      " 'margot.m4a_6.wav' - a cry sound.\n",
      " 'margot.m4a_7.wav' - a cry sound.\n",
      " 'margot.m4a_8.wav' - a cry sound.\n",
      " 'margot.m4a_9.wav' - a cry sound.\n",
      "'silence.wav_0.wav' - not a cry sound.\n",
      "'silence.wav_1.wav' - not a cry sound.\n",
      "'silence.wav_10.wav' - not a cry sound.\n",
      "'silence.wav_100.wav' - not a cry sound.\n",
      "'silence.wav_101.wav' - not a cry sound.\n",
      "'silence.wav_102.wav' - not a cry sound.\n",
      "'silence.wav_103.wav' - not a cry sound.\n",
      "'silence.wav_104.wav' - not a cry sound.\n",
      "'silence.wav_105.wav' - not a cry sound.\n",
      "'silence.wav_106.wav' - not a cry sound.\n",
      "'silence.wav_107.wav' - not a cry sound.\n",
      "'silence.wav_11.wav' - not a cry sound.\n",
      "'silence.wav_12.wav' - not a cry sound.\n",
      "'silence.wav_13.wav' - not a cry sound.\n",
      "'silence.wav_14.wav' - not a cry sound.\n",
      "'silence.wav_15.wav' - not a cry sound.\n",
      "'silence.wav_16.wav' - not a cry sound.\n",
      "'silence.wav_17.wav' - not a cry sound.\n",
      "'silence.wav_18.wav' - not a cry sound.\n",
      "'silence.wav_19.wav' - not a cry sound.\n",
      "'silence.wav_2.wav' - not a cry sound.\n",
      "'silence.wav_20.wav' - not a cry sound.\n",
      "'silence.wav_21.wav' - not a cry sound.\n",
      "'silence.wav_22.wav' - not a cry sound.\n",
      "'silence.wav_23.wav' - not a cry sound.\n",
      "'silence.wav_24.wav' - not a cry sound.\n",
      "'silence.wav_25.wav' - not a cry sound.\n",
      "'silence.wav_26.wav' - not a cry sound.\n",
      "'silence.wav_27.wav' - not a cry sound.\n",
      "'silence.wav_28.wav' - not a cry sound.\n",
      "'silence.wav_29.wav' - not a cry sound.\n",
      "'silence.wav_3.wav' - not a cry sound.\n",
      "'silence.wav_30.wav' - not a cry sound.\n",
      "'silence.wav_31.wav' - not a cry sound.\n",
      "'silence.wav_32.wav' - not a cry sound.\n",
      "'silence.wav_33.wav' - not a cry sound.\n",
      "'silence.wav_34.wav' - not a cry sound.\n",
      "'silence.wav_35.wav' - not a cry sound.\n",
      "'silence.wav_36.wav' - not a cry sound.\n",
      "'silence.wav_37.wav' - not a cry sound.\n",
      "'silence.wav_38.wav' - not a cry sound.\n",
      "'silence.wav_39.wav' - not a cry sound.\n",
      "'silence.wav_4.wav' - not a cry sound.\n",
      "'silence.wav_40.wav' - not a cry sound.\n",
      "'silence.wav_41.wav' - not a cry sound.\n",
      "'silence.wav_42.wav' - not a cry sound.\n",
      "'silence.wav_43.wav' - not a cry sound.\n",
      "'silence.wav_44.wav' - not a cry sound.\n",
      "'silence.wav_45.wav' - not a cry sound.\n",
      "'silence.wav_46.wav' - not a cry sound.\n",
      "'silence.wav_47.wav' - not a cry sound.\n",
      "'silence.wav_48.wav' - not a cry sound.\n",
      "'silence.wav_49.wav' - not a cry sound.\n",
      "'silence.wav_5.wav' - not a cry sound.\n",
      "'silence.wav_50.wav' - not a cry sound.\n",
      "'silence.wav_51.wav' - not a cry sound.\n",
      "'silence.wav_52.wav' - not a cry sound.\n",
      "'silence.wav_53.wav' - not a cry sound.\n",
      "'silence.wav_54.wav' - not a cry sound.\n",
      "'silence.wav_55.wav' - not a cry sound.\n",
      "'silence.wav_56.wav' - not a cry sound.\n",
      "'silence.wav_57.wav' - not a cry sound.\n",
      "'silence.wav_58.wav' - not a cry sound.\n",
      "'silence.wav_59.wav' - not a cry sound.\n",
      "'silence.wav_6.wav' - not a cry sound.\n",
      "'silence.wav_60.wav' - not a cry sound.\n",
      "'silence.wav_61.wav' - not a cry sound.\n",
      "'silence.wav_62.wav' - not a cry sound.\n",
      "'silence.wav_63.wav' - not a cry sound.\n",
      "'silence.wav_64.wav' - not a cry sound.\n",
      "'silence.wav_65.wav' - not a cry sound.\n",
      " 'silence.wav_66.wav' - a cry sound.\n",
      " 'silence.wav_67.wav' - a cry sound.\n",
      " 'silence.wav_68.wav' - a cry sound.\n",
      " 'silence.wav_69.wav' - a cry sound.\n",
      "'silence.wav_7.wav' - not a cry sound.\n",
      " 'silence.wav_70.wav' - a cry sound.\n",
      "'silence.wav_71.wav' - not a cry sound.\n",
      "'silence.wav_72.wav' - not a cry sound.\n",
      "'silence.wav_73.wav' - not a cry sound.\n",
      "'silence.wav_74.wav' - not a cry sound.\n",
      "'silence.wav_75.wav' - not a cry sound.\n",
      "'silence.wav_76.wav' - not a cry sound.\n",
      "'silence.wav_77.wav' - not a cry sound.\n",
      "'silence.wav_78.wav' - not a cry sound.\n",
      "'silence.wav_79.wav' - not a cry sound.\n",
      "'silence.wav_8.wav' - not a cry sound.\n",
      "'silence.wav_80.wav' - not a cry sound.\n",
      "'silence.wav_81.wav' - not a cry sound.\n",
      "'silence.wav_82.wav' - not a cry sound.\n",
      "'silence.wav_83.wav' - not a cry sound.\n",
      "'silence.wav_84.wav' - not a cry sound.\n",
      "'silence.wav_85.wav' - not a cry sound.\n",
      "'silence.wav_86.wav' - not a cry sound.\n",
      "'silence.wav_87.wav' - not a cry sound.\n",
      "'silence.wav_88.wav' - not a cry sound.\n",
      "'silence.wav_89.wav' - not a cry sound.\n",
      "'silence.wav_9.wav' - not a cry sound.\n",
      "'silence.wav_90.wav' - not a cry sound.\n",
      "'silence.wav_91.wav' - not a cry sound.\n",
      "'silence.wav_92.wav' - not a cry sound.\n",
      "'silence.wav_93.wav' - not a cry sound.\n",
      "'silence.wav_94.wav' - not a cry sound.\n",
      "'silence.wav_95.wav' - not a cry sound.\n",
      "'silence.wav_96.wav' - not a cry sound.\n",
      "'silence.wav_97.wav' - not a cry sound.\n",
      "'silence.wav_98.wav' - not a cry sound.\n",
      "'silence.wav_99.wav' - not a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_0.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_1.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_10.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_11.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_12.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_13.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_14.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_15.wav' - a cry sound.\n",
      "'V_2017-04-01+08_06_22=0_30.mp3_16.wav' - not a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_17.wav' - a cry sound.\n",
      "'V_2017-04-01+08_06_22=0_30.mp3_18.wav' - not a cry sound.\n",
      "'V_2017-04-01+08_06_22=0_30.mp3_19.wav' - not a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_2.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_20.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_21.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_22.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_23.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_24.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_25.wav' - a cry sound.\n",
      "'V_2017-04-01+08_06_22=0_30.mp3_3.wav' - not a cry sound.\n",
      "'V_2017-04-01+08_06_22=0_30.mp3_4.wav' - not a cry sound.\n",
      "'V_2017-04-01+08_06_22=0_30.mp3_5.wav' - not a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_6.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_7.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_8.wav' - a cry sound.\n",
      " 'V_2017-04-01+08_06_22=0_30.mp3_9.wav' - a cry sound.\n"
     ]
    }
   ],
   "source": [
    "# Folder containing the audio clips you want to classify\n",
    "folder_path = r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\New folder\"\n",
    "\n",
    "# List all audio files in the folder\n",
    "audio_files = [f for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n",
    "\n",
    "# Classify each audio clip in the folder\n",
    "for audio_file in audio_files:\n",
    "    # Get the full path to the audio file\n",
    "    audio_path = os.path.join(folder_path, audio_file)\n",
    "\n",
    "    # Extract features from the audio clip\n",
    "    new_feature_vector = extract_features(audio_path)\n",
    "\n",
    "    # Make a prediction using the trained model\n",
    "    prediction = clf.predict([new_feature_vector])\n",
    "\n",
    "    # Interpret the prediction\n",
    "    if prediction[0] == 1:\n",
    "        print(f\" '{audio_file}' - a cry sound.\")\n",
    "    else:\n",
    "        print(f\"'{audio_file}' - not a cry sound.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "16dfda9a",
   "metadata": {},
   "source": [
    "Train a Support Vector Machine (SVM) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d882079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer  # Import SimpleImputer for handling missing values\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "eed7e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import spectrogram\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import skew, kurtosis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4416b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9444444444444444\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96        23\n",
      "           1       0.92      0.92      0.92        13\n",
      "\n",
      "    accuracy                           0.94        36\n",
      "   macro avg       0.94      0.94      0.94        36\n",
      "weighted avg       0.94      0.94      0.94        36\n",
      "\n",
      "Predicted Label for the New Audio Clip: 1\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in X using SimpleImputer (replace NaNs with the mean of each feature)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "# Convert the target variable to integers (0 for non-cry, 1 for cry)\n",
    "y = y.astype(int)  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a Support Vector Machine (SVM) Classifier\n",
    "clf = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "\n",
    "# Convert the target variable to integers (0 for non-cry, 1 for cry)\n",
    "y = y.astype(int)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Now, you can use X_test_df for prediction\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Now, for testing a new audio clip:\n",
    "new_audio_path = r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\301 - Crying baby\\V_2017-04-01+08_06_22=0_30.mp3_12.wav\"\n",
    "\n",
    "#baby laugh\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\903 - Baby laugh\\laugh_1.m4a_9.wav\"\n",
    "\n",
    "#baby cry\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\301 - Crying baby\\V_2017-04-01+08_06_22=0_30.mp3_12.wav\"\n",
    "\n",
    "#silence\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\901 - Silence\\silence.wav_8.wav\"\n",
    "\n",
    "# Read the new audio file\n",
    "fs, new_audio_signal = wavfile.read(new_audio_path)\n",
    "\n",
    "# Compute STFT for the new audio clip (similar to the training data)\n",
    "f, t, Sxx = spectrogram(new_audio_signal, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "\n",
    "# Calculate the 20 features from STFT\n",
    "feature_vector = extract_features(new_audio_path)\n",
    "\n",
    "\n",
    "# Here, we assume you have a function extract_features(Sxx) that computes the features\n",
    "# feature_vector = extract_features(Sxx)\n",
    "\n",
    "# Handle missing values in the feature vector using SimpleImputer\n",
    "feature_vector = imputer.transform([feature_vector])\n",
    "\n",
    "# Use the trained SVM classifier to predict the label for the new audio clip\n",
    "predicted_label = clf.predict(feature_vector)\n",
    "\n",
    "# Display the predicted label (1 for cry, 0 for non-cry)\n",
    "print(\"Predicted Label for the New Audio Clip:\", predicted_label[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7b2a9",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a7d39272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2eef6b72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8863636363636364\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91       108\n",
      "           1       0.91      0.78      0.84        68\n",
      "\n",
      "    accuracy                           0.89       176\n",
      "   macro avg       0.89      0.87      0.88       176\n",
      "weighted avg       0.89      0.89      0.88       176\n",
      "\n",
      "Predicted Label for the New Audio Clip: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train a Logistic Regression Classifier\n",
    "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "# Evaluate the model (this is for training evaluation; you should split your data for proper evaluation)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "report = classification_report(y, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_path = \"cry_detection_logistic_regression_model.pkl\"\n",
    "joblib.dump(clf, model_path)\n",
    "\n",
    "# Now, for testing a new audio clip:\n",
    "new_audio_path = r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\901 - Silence\\silence.wav_8.wav\"\n",
    "#baby laugh\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\903 - Baby laugh\\laugh_1.m4a_9.wav\"\n",
    "\n",
    "#baby cry\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\301 - Crying baby\\V_2017-04-01+08_06_22=0_30.mp3_12.wav\"\n",
    "\n",
    "#silence\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\901 - Silence\\silence.wav_8.wav\"\n",
    "\n",
    "# Read the new audio file\n",
    "fs, new_audio_signal = wavfile.read(new_audio_path)\n",
    "\n",
    "# Compute STFT for the new audio clip (similar to the training data)\n",
    "f, t, Sxx = spectrogram(new_audio_signal, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "\n",
    "# Calculate the 20 features from STFT\n",
    "feature_vector = extract_features(new_audio_path)\n",
    "\n",
    "\n",
    "# Here, we assume you have a function extract_features(Sxx) that computes the features\n",
    "# feature_vector = extract_features(Sxx)\n",
    "\n",
    "# Ensure that feature_vector contains 20 elements\n",
    "\n",
    "# Handle missing values in the feature vector using SimpleImputer (use the same imputer from before)\n",
    "feature_vector = imputer.transform([feature_vector])\n",
    "\n",
    "# Use the trained Logistic Regression classifier to predict the label for the new audio clip\n",
    "predicted_label = clf.predict(feature_vector)\n",
    "\n",
    "# Display the predicted label (1 for cry, 0 for non-cry)\n",
    "print(\"Predicted Label for the New Audio Clip:\", predicted_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2ff84a",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (K-NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f99da422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "108e34a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8611111111111112\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.91      0.89        23\n",
      "         1.0       0.83      0.77      0.80        13\n",
      "\n",
      "    accuracy                           0.86        36\n",
      "   macro avg       0.85      0.84      0.85        36\n",
      "weighted avg       0.86      0.86      0.86        36\n",
      "\n",
      "Predicted Label for the New Audio Clip: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Ensure X is in a suitable format (NumPy array)\n",
    "X = np.array(X)\n",
    "\n",
    "# Check if X is C-contiguous\n",
    "if not X.flags.c_contiguous:\n",
    "    # If not C-contiguous, create a new C-contiguous array and copy the data\n",
    "    X = np.ascontiguousarray(X)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train a K-Nearest Neighbors (K-NN) Classifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)  # You can adjust the number of neighbors as needed\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_path = \"cry_detection_model_knn.pkl\"\n",
    "joblib.dump(clf, model_path)\n",
    "\n",
    "# Now, for testing a new audio clip:\n",
    "new_audio_path = r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\301 - Crying baby\\V_2017-04-01+08_06_22=0_30.mp3_12.wav\"\n",
    "\n",
    "#baby laugh\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\903 - Baby laugh\\laugh_1.m4a_9.wav\"\n",
    "\n",
    "#baby cry\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\301 - Crying baby\\V_2017-04-01+08_06_22=0_30.mp3_12.wav\"\n",
    "\n",
    "#silence\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\901 - Silence\\silence.wav_8.wav\"\n",
    "\n",
    "# Read the new audio file\n",
    "fs, new_audio_signal = wavfile.read(new_audio_path)\n",
    "\n",
    "# Compute STFT for the new audio clip (similar to the training data)\n",
    "f, t, Sxx = spectrogram(new_audio_signal, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "\n",
    "# Calculate the 20 features from STFT\n",
    "feature_vector = extract_features(new_audio_path)\n",
    "\n",
    "\n",
    "# Here, we assume you have a function extract_features(Sxx) that computes the features\n",
    "# feature_vector = extract_features(Sxx)\n",
    "\n",
    "# Ensure that feature_vector contains 20 elements\n",
    "\n",
    "# Handle missing values in the feature vector using SimpleImputer (use the same imputer from before)\n",
    "feature_vector = imputer.transform([feature_vector])\n",
    "\n",
    "# Use the trained Logistic Regression classifier to predict the label for the new audio clip\n",
    "predicted_label = clf.predict(feature_vector)\n",
    "\n",
    "# Display the predicted label (1 for cry, 0 for non-cry)\n",
    "print(\"Predicted Label for the New Audio Clip:\", predicted_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406c657",
   "metadata": {},
   "source": [
    "Gradient Boosting (e.g., XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e7ee3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d8b7092f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        23\n",
      "         1.0       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        36\n",
      "   macro avg       1.00      1.00      1.00        36\n",
      "weighted avg       1.00      1.00      1.00        36\n",
      "\n",
      "Predicted Label for the New Audio Clip: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\cry_det\\Lib\\site-packages\\xgboost\\data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train an XGBoost Classifier\n",
    "clf = xgb.XGBClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_path = \"cry_detection_xgboost_model.pkl\"\n",
    "clf.save_model(model_path)\n",
    "\n",
    "# Now, for testing a new audio clip:\n",
    "new_audio_path = r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\301 - Crying baby\\V_2017-04-01+08_06_22=0_30.mp3_12.wav\"\n",
    "\n",
    "#baby laugh\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\903 - Baby laugh\\laugh_1.m4a_9.wav\"\n",
    "\n",
    "#baby cry\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\301 - Crying baby\\V_2017-04-01+08_06_22=0_30.mp3_12.wav\"\n",
    "\n",
    "#silence\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\901 - Silence\\silence.wav_8.wav\"\n",
    "\n",
    "# Read the new audio file\n",
    "fs, new_audio_signal = wavfile.read(new_audio_path)\n",
    "\n",
    "# Compute STFT for the new audio clip (similar to the training data)\n",
    "f, t, Sxx = spectrogram(new_audio_signal, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "\n",
    "# Calculate the 20 features from STFT\n",
    "feature_vector = extract_features(new_audio_path)\n",
    "\n",
    "\n",
    "# Here, we assume you have a function extract_features(Sxx) that computes the features\n",
    "# feature_vector = extract_features(Sxx)\n",
    "\n",
    "# Ensure that feature_vector contains 20 elements\n",
    "\n",
    "# Handle missing values in the feature vector using SimpleImputer (use the same imputer from before)\n",
    "feature_vector = imputer.transform([feature_vector])\n",
    "\n",
    "# Use the trained Logistic Regression classifier to predict the label for the new audio clip\n",
    "predicted_label = clf.predict(feature_vector)\n",
    "\n",
    "# Display the predicted label (1 for cry, 0 for non-cry)\n",
    "print(\"Predicted Label for the New Audio Clip:\", predicted_label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d8788f",
   "metadata": {},
   "source": [
    "Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "376ea1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "a07afda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 92ms/step - loss: 0.7204 - accuracy: 0.5893 - val_loss: 0.5731 - val_accuracy: 0.7857\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.5552 - accuracy: 0.8304 - val_loss: 0.4483 - val_accuracy: 0.8571\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.4385 - accuracy: 0.9286 - val_loss: 0.3571 - val_accuracy: 0.9643\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.3554 - accuracy: 0.9821 - val_loss: 0.2901 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 0.2913 - accuracy: 1.0000 - val_loss: 0.2373 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 0.2410 - accuracy: 1.0000 - val_loss: 0.1934 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.1972 - accuracy: 1.0000 - val_loss: 0.1570 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 0.1612 - accuracy: 1.0000 - val_loss: 0.1265 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.1313 - accuracy: 1.0000 - val_loss: 0.1006 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.1056 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 1.0000\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1311 - accuracy: 1.0000\n",
      "Test Accuracy: 1.0\n",
      "Predicted Label for the New Audio Clip: 1\n"
     ]
    }
   ],
   "source": [
    "# Build a neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(20),  # Input layer with the number of features\n",
    "    keras.layers.Dense(64, activation='relu'),  # Hidden layer with 64 units and ReLU activation\n",
    "    keras.layers.Dense(32, activation='relu'),  # Hidden layer with 32 units and ReLU activation\n",
    "    keras.layers.Dense(1, activation='sigmoid')  # Output layer with 1 unit and sigmoid activation\n",
    "])\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"infant_cry_detection_model.h5\")\n",
    "\n",
    "# Now, for testing a new audio clip:\n",
    "new_audio_path = r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\301 - Crying baby\\V_2017-04-01+08_06_22=0_30.mp3_12.wav\"\n",
    "\n",
    "#baby laugh\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\903 - Baby laugh\\laugh_1.m4a_9.wav\"\n",
    "\n",
    "#baby cry\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\301 - Crying baby\\V_2017-04-01+08_06_22=0_30.mp3_12.wav\"\n",
    "\n",
    "#silence\n",
    "#r\"D:\\infant cry detecting\\dev\\baby_cry\\baby_cry_detection\\data\\901 - Silence\\silence.wav_8.wav\"\n",
    "\n",
    "# Read the new audio file\n",
    "fs, new_audio_signal = wavfile.read(new_audio_path)\n",
    "\n",
    "# Compute STFT for the new audio clip (similar to the training data)\n",
    "f, t, Sxx = spectrogram(new_audio_signal, fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "\n",
    "# Calculate the 20 features from STFT\n",
    "feature_vector = extract_features(new_audio_path)\n",
    "\n",
    "\n",
    "# Here, we assume you have a function extract_features(Sxx) that computes the features\n",
    "# feature_vector = extract_features(Sxx)\n",
    "\n",
    "# Ensure that feature_vector contains 20 elements\n",
    "\n",
    "# Handle missing values in the feature vector using SimpleImputer (use the same imputer from before)\n",
    "feature_vector = imputer.transform([feature_vector])\n",
    "\n",
    "# Use the trained Logistic Regression classifier to predict the label for the new audio clip\n",
    "predicted_label = clf.predict(feature_vector)\n",
    "\n",
    "# Display the predicted label (1 for cry, 0 for non-cry)\n",
    "print(\"Predicted Label for the New Audio Clip:\", predicted_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bb497f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
